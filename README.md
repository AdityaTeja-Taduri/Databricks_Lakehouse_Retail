# Databricks Lakehouse Retail ETL Pipeline

This project shows how to build a complete **Lakehouse ETL pipeline** on Databricks using the Bronze â†’ Silver â†’ Gold architecture.  
I used Unity Catalog, Auto Loader, Delta Lake, and Databricks Volumes to create a realistic, production-style data engineering workflow.

---

## ğŸ› ï¸ Tech Stack

- **Databricks Lakehouse Platform**
- **Unity Catalog** (governance, schema management, permissions)
- **Delta Lake** (Bronze â†’ Silver â†’ Gold storage)
- **PySpark** (ETL transformations)
- **Auto Loader** (streaming ingestion)
- **Photon Engine** (optimized execution)
- **SQL** (views & business aggregations)
- **MLflow** (optional lineage and tracking)

---

## ğŸš€ What This Project Does

### 1. **Generates a retail dataset**
- Creates ~100,000 fake retail orders  
- Saves them as multiple CSV files in a Databricks Volume  
- Acts like a â€œraw data landing zoneâ€

### 2. **Bronze Layer â€“ Ingestion with Auto Loader**
- Reads the raw CSVs incrementally  
- Uses schema inference + checkpoints  
- Stores clean Delta files in the Bronze layer

### 3. **Silver Layer â€“ Cleaning & Validation**
- Corrects datatypes  
- Removes invalid or missing records  
- Adds useful fields like `order_date`, `net_amount`, etc.

### 4. **Gold Layer â€“ Business Tables**
Creates 3 analytics tables:
- **Daily Sales**  
- **Top Categories**  
- **Customer Lifetime Value**

---

## ğŸ“ Repository Structure

```
databricks-lakehouse-retail/
â”‚
â”œâ”€â”€ notebooks/                       # Python versions of all notebooks
â”‚   â”œâ”€â”€ 00_generate_retail_raw.py
â”‚   â”œâ”€â”€ 01_bronze_autoloader.py
â”‚   â”œâ”€â”€ 02_silver_cleaning.py
â”‚   â””â”€â”€ 03_gold_business.py
â”‚
â”œâ”€â”€ databricks/                      # Databricks-native notebook exports
â”‚   â”œâ”€â”€ 00_generate_retail_raw.dbc
â”‚   â”œâ”€â”€ 01_bronze_autoloader.dbc
â”‚   â”œâ”€â”€ 02_silver_cleaning.dbc
â”‚   â””â”€â”€ 03_gold_business.dbc
â”‚
â”œâ”€â”€ sql/                             # SQL view layer 
â”‚   â”œâ”€â”€ daily_sales_view.sql
â”‚   â”œâ”€â”€ top_categories_view.sql
â”‚   â””â”€â”€ customer_ltv_view.sql
â”‚
â”œâ”€â”€ screenshots/                     # Visuals from the Databricks workspace
â”‚   â”œâ”€â”€ catalog_structure.png
â”‚   â”œâ”€â”€ bronze_preview.png
â”‚   â”œâ”€â”€ silver_preview.png
â”‚   â”œâ”€â”€ gold_preview.png
â”‚   â””â”€â”€ architecture_diagram.png
â”‚
â”œâ”€â”€ architecture/                    # Architecture diagram(s)
â”‚   â””â”€â”€ lakehouse_diagram.png
â”‚
â””â”€â”€ README.md                        # Main documentation file
```


---

## ğŸ§° Tools Used

- **Databricks (Free Edition)**
- **Unity Catalog**
- **Databricks Volumes**
- **Auto Loader (cloudFiles)**
- **Delta Lake**
- **PySpark**

---

## â–¶ï¸ How To Run This Project

1. Create the `bronze`, `silver`, and `gold` schemas in Unity Catalog.  
2. Create a Volume inside the `bronze` schema (named `raw_retail`).  
3. Run notebook **00** to generate data.  
4. Run notebook **01** to ingest data with Auto Loader.  
5. Run notebook **02** to clean the data.  
6. Run notebook **03** to produce business tables.


---

## ğŸ’¼ Why This Project Matters

This project mirrors what Data Engineers do in real companies:
- Creating pipelines  
- Managing schemas  
- Cleaning real-world messy data  
- Building analytical tables  
- Organizing code in a clear project structure  

---

## ğŸ“¸ Screenshots

Screenshots of the catalog, Bronze/Silver/Gold tables, and pipeline flow are included in the `screenshots/` folder.

---

## ğŸ§± Architecture

This project follows the Databricks Lakehouse medallion pattern:

- **Storage:** Databricks Volumes in Unity Catalog (`/Volumes/workspace/bronze/raw_retail`)
- **Bronze:** Auto Loader ingests raw CSV files into `workspace.bronze.retail_orders_bronze`
- **Silver:** Cleaning, type casting, validation, and enrichment into `workspace.silver.retail_orders_silver`
- **Gold:** Business-ready tables:
  - `workspace.gold.daily_sales`
  - `workspace.gold.top_categories`
  - `workspace.gold.customer_lifetime_value`

High-level flow:

```text
Raw CSV (Volume)
/Volumes/workspace/bronze/raw_retail
                â”‚
                â–¼
        Auto Loader (Bronze)
 workspace.bronze.retail_orders_bronze
                â”‚
                â–¼
     Cleaning & Enrichment (Silver)
 workspace.silver.retail_orders_silver
                â”‚
                â–¼
       Gold Business Marts (Gold)
   daily_sales Â· top_categories Â· customer_lifetime_value


---

